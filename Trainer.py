#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import random

from MCTS import MCTS
from Arena import Arena

class Trainer(object):
    '''
    このクラスは，セルフプレイ・学習を行う．流れは以下の通り．
    1:episodeを実行する．各step毎に(canonical_state,player,pi,None)のリストをexperienceに追加．
    2:episode終了時に，勝敗に基づいてexperienceに書き込み
    3:1~2をnum_episodes繰り返して，experienceを蓄積する．
    4:experienceからランダムにmini_batch_sizeだけ取り出す．
    5:現在のNNのコピーを作成し，それに対して4を用いて訓練．
    6：5の結果と元のNNとでn_compare回だけ対戦する．
    7：元のNNに対する勝率が55%以上ならば採用する．かつNNを保存する．
    8：1~8をn_iter回繰り返す．
    '''
    
    
    ###########################################################################
    def __init__(self,env,env_utils,model_system,args):
        self.env=env
        self.env_utils=env_utils
        
        self.new_model_system=model_system
        self.cur_model_system=self.new_model_system.__class__(self.env)
        
        self.mcts=MCTS(self.env_utils,self.new_model_system,args)
        
        self.experiences=[]
        
        self.args=args
    
    
    ###########################################################################
    def run_episode(self):
        '''
        Input
        
        Return:
            experience:N*(canonical_state,pi,v)
            pi is policy vector generated by MCTS
            v is final game result (win +1, lose -1)
        '''
        experience=[]
        
        env=self.env
        env.reset()
        
        step=0 #temperature等の探索の制御に用いる．
        while True:
            step+=1
            canonical_state=self.env_utils.get_canonical_form(env.state,env.player)
            temp=int(step<self.args.temp_threshold) #ボルツマン温度
            
            #MCTS方策取得
            pi=self.mcts.get_action_prob(canonical_state,temp=temp)
            
            #対称性によるデータ拡張
            sym=self.env_utils.get_symmetries(canonical_state,pi)
            for cs,p in sym:
                #print(p)
                experience.append([cs,env.player,p,None])            
            
            #実際に行動を選択して実行
            action=np.random.choice(len(pi),p=pi)
            _,_,r,_,_=env.step(action)
            
            #終局したら，結果をexperienceに書き込んで，返す．
            if r!=0:
                return [(x[0],x[2],r if x[1]==env.player else -r) for x in experience]
    
    
    ###########################################################################
    def train(self):
        '''
        '''
        for i in range(1,self.args.num_iters+1):
            print('----START ITERATION:'+str(i))
            
            #num_episodes回対戦して，experienceを蓄積する．
            for _ in range(self.args.num_episodes):
                self.mcts=MCTS(self.env_utils,self.new_model_system,self.args) #MCTS初期化
                self.experiences.extend(self.run_episode())
                
                if len(self.experiences)>self.args.max_experiences:
                    self.experiences \
                        =self.experiences[len(self.experiences)-self.args.max_experiences:]
            
            #experienceからランダムサンプリングして，NN用のミニバッチを作る．
            mini_batch=random.sample(self.experiences,min([len(self.experiences),self.args.mini_batch_size]))
            
            #現在のモデルを保存して，cur_model_systemに読み込み．
            self.new_model_system.save_checkpoint(folder=self.args.checkpoint,filename='temp.pth.tar')
            self.cur_model_system.load_checkpoint(folder=self.args.checkpoint,filename='temp.pth.tar')
            cur_mcts=MCTS(self.env_utils,self.cur_model_system,self.args)
            
            self.new_model_system.fit(mini_batch)
            new_mcts=MCTS(self.env_utils,self.new_model_system,self.args)
            
            #Arenaで対戦させて，既存モデルと比べて強くなったか確認する．
            print('PITTING AGAINST PREVIOUS VERSION')
            arena=Arena(lambda x:np.argmax(cur_mcts.get_action_prob(x,temp=0)),
                        lambda x:np.argmax(new_mcts.get_action_prob(x,temp=0)),
                        self.env,display=False)
            cur_wins,new_wins,draws=arena.play_games(self.args.num_compares_arena)
            print('NEW/CUR WINS : %d / %d ; DRAWS : %d' % (new_wins,cur_wins,draws))
            
            #update_thresholdよりも強くなっていたら，採用，強くなっていなかったら棄却する．
            if 0<cur_wins+new_wins and float(new_wins)/(cur_wins+new_wins)<self.args.update_threshold:
                print('REJECTING NEW MODEL')
                self.new_model_system.load_checkpoint(folder=self.args.checkpoint,filename='temp.pth.tar')
            else:
                print('ACCEPTING NEW MODEL')
                self.new_model_system.save_checkpoint(folder=self.args.checkpoint,filename=self.get_checkpoint_filename(i))
                self.new_model_system.save_checkpoint(folder=self.args.checkpoint,filename='best.pth.tar')
    
    
    ###########################################################################
    def get_checkpoint_filename(self,iteration):
        return 'checkpoint_'+str(iteration)+'.pth.tar'
    
    